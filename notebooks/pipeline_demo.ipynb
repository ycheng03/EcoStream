{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Yanyu (Claudia) Cheng\n",
    "# This is the modular, production-ready version of the analysis script\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Global plot configuration\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "CONFIG = {\n",
    "    \"time_cols\": [\"year\", \"month\", \"day\", \"hour\"],\n",
    "    \"features\": [\"PM2.5\", \"PM10\", \"TEMP\", \"PRES\", \"DEWP\"],\n",
    "}\n",
    "\n",
    "def ingest_and_process_data(file_path):\n",
    "    # Ingests raw sensor data, parses timestamps, and aggregates to daily mean\n",
    "    try:\n",
    "        print(f\"Loading data from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Validate columns exist before processing\n",
    "        required_cols = CONFIG['time_cols'] + CONFIG['features']\n",
    "        if not set(required_cols).issubset(df.columns):\n",
    "            missing = set(required_cols) - set(df.columns)\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        # Parse datetime\n",
    "        df['timestamp'] = pd.to_datetime(df[CONFIG['time_cols']])\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Select and clean features, using .copy() ensures it doesn't trigger SettingWithCopy warnings\n",
    "        df_clean = df[CONFIG['features']].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        \n",
    "        # Report data loss\n",
    "        initial_rows = len(df)\n",
    "        clean_rows = len(df_clean)\n",
    "        print(f\"Data Cleaning: Dropped {initial_rows - clean_rows} rows ({(initial_rows - clean_rows)/initial_rows:.1%} loss)\")\n",
    "\n",
    "        # Downsample to daily mean\n",
    "        df_daily = df_clean.resample('D').mean()\n",
    "        return df_daily\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error during ingestion: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_anomalies(df_in, contamination=0.05):\n",
    "    # Trains an Isolation Forest to flag multivariate anomalies\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # Contamination set to 5% based on reasonable starting assumption sensor failure rate \n",
    "    model = IsolationForest(contamination=contamination, random_state=42)\n",
    "    \n",
    "    df['anomaly_score'] = model.fit_predict(df)\n",
    "    # Isolation Forest returns -1 for anomalies, 1 for normal\n",
    "    df['is_anomaly'] = df['anomaly_score'] == -1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def visualize_results(df):\n",
    "    \"\"\"Generates diagnostic plots for the anomaly detection.\"\"\"\n",
    "    \n",
    "    # Plot 1: Time Series with highlighted anomalies\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.lineplot(data=df, x=df.index, y='PM2.5', label='PM2.5 Conc.', alpha=0.6)\n",
    "    \n",
    "    anomalies = df[df['is_anomaly']]\n",
    "    plt.scatter(anomalies.index, anomalies['PM2.5'], color='red', label='Multivariate Anomaly', s=30, marker='x', zorder=5)\n",
    "    \n",
    "    plt.title('PM2.5 Time Series with Multivariate Anomalies (Isolation Forest)')\n",
    "    plt.ylabel('Concentration (ug/m3)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Correlation Heatmap\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(df[CONFIG['features']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution Block\n",
    "def run_pipeline(file_path='../data/raw_data.csv'):\n",
    "    \n",
    "    print(\"--- Starting Pipeline ---\")\n",
    "    df_processed = ingest_and_process_data(file_path)\n",
    "    \n",
    "    if df_processed is not None:\n",
    "        # Inspect Shape\n",
    "        print(f\"Dataset Shape: {df_processed.shape}\")\n",
    "        \n",
    "        # Model Training\n",
    "        print(\"\\n--- Training Model ---\")\n",
    "        df_results = detect_anomalies(df_processed)\n",
    "        \n",
    "        n_anomalies = df_results['is_anomaly'].sum()\n",
    "        print(f\"Anomalies Detected: {n_anomalies} ({n_anomalies/len(df_results):.1%})\")\n",
    "        \n",
    "        # Results Preview Table\n",
    "        print(\"\\nResults Preview (Head):\")\n",
    "        preview = df_results[['PM2.5', 'TEMP', 'is_anomaly']].head().reset_index()\n",
    "        preview = preview.rename(columns={preview.columns[0]: 'date'})\n",
    "        preview = preview.round({'PM2.5':4, 'TEMP':4})\n",
    "        print(preview.to_string(index=False))\n",
    "\n",
    "\n",
    "        # Visualize\n",
    "        print(\"\\n--- Generating Visuals ---\")\n",
    "        visualize_results(df_results)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not load data from: {file_path}\")\n",
    "\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0389173",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
